{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: All code should be in Python3. Keras version should be 2.0.4. The directory structure on the bitbucket repo should be exactly same as the hw4.zip provided to you (with the exception of data directory. Do not upload it). To push the code to remote repo, use the same instructions as given in HW0. **Double check you remote repo for correct directory structure. We won't consider any regrade requests based on wrong directory structure penalty. Again, do not upload data to your bitbucket repo ** <br>\n",
    "**The data provided to you should not be used for any other purpose than this course. You must not distribute it or upload it to any public platform.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we are going to solve the problem of summarization using a sequence to sequence model. In a sequence to sequence problem, we have an encoder and a decoder. We feed the sequence of word embeddings to an encoder and train decoder to learn the summaries. We will be seeing 2 types of encoder decoder architectures in this assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the assignment is to prepare data. You are given training data in train_article.txt, in which each line is the first sentence from an article, and training summary sentences in train_title.txt, which are the corresponding titles of the article. You will be training the model to predict the title of an article given the first sentence of that article, where title generation is a summarization task. Let us limit the maximum vocabulary size to 20000 and maximum length of article to 200 (These are just initial params to get you started and we recommend experimenting, to improve your scores after you are done with your first implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 200\n",
    "VOCAB_SIZE = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which takes article file, summary file, maximum length of sentence and vocabulary size and does the following\n",
    "* Create vocabulary: Take most frequent VOCAB_SIZE number of words from article file. Add two special symbols ZERO at start and UNK at end to finally have VOCAB_SIZE + 2 words. Use this array as idx2word. Repeat the process for summary data to create another idx2word corresponding to it. \n",
    "* Using the above idx2word for both article and summary data, create word2idx, which will map every word to its index in idx2word array. \n",
    "* Convert the words in the article and summary data to their corresponding index from word2idx. If a word is not present in the vocab, use the index of UNK. \n",
    "* After the above preprocessing, each sentence in article and summary data should be a list of indices\n",
    "* Now find the max length of a sentence (which is basically number of indices in a sentence) in article data. Pad every sentence in article data to that length, so that all sentences are of same length. You may use pad_sequences function provided by keras. Do the same for title data.\n",
    "* return the following outputs transformed article data, vocab size of article data, idx2word(articledata), word2idx(articledata),transformed summary data, vocab size of summary data, idx2word(summarydata), word2idx(summarydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 22, 5, 25, 73, 3, 14, 7, 78]\n",
      "[78, 530, 13, 188, 8]\n"
     ]
    }
   ],
   "source": [
    "#complete this function using details as above\n",
    "\n",
    "import re\n",
    "import collections\n",
    "\n",
    "#def load_data(article, summary, max_len, vocab_size):\n",
    "def load_data(article, summary, max_len, vocab_size):\n",
    "    # Create vocabulary for article file\n",
    "    with open(article) as f_article:\n",
    "        words_article = f_article.read()\n",
    "    \n",
    "    words_article = re.findall(r'\\w+', words_article)\n",
    "    word_counts_article = dict(Counter(words_article).most_common(vocab_size))\n",
    "    \n",
    "    idx2word_article = list(word_counts_article.keys())\n",
    "    idx2word_article.insert(0,\"ZERO\")\n",
    "    idx2word_article.append(\"UNK\")\n",
    "    \n",
    "    # Update vocab_size\n",
    "    vocab_size = vocab_size + 2\n",
    "    \n",
    "    # Create vocabulary for summary file\n",
    "    with open(summary) as f_title:\n",
    "        words_title = f_title.read()\n",
    "    \n",
    "    words_title = re.findall(r'\\w+', words_title)\n",
    "    word_counts_title = dict(Counter(words_title).most_common(vocab_size))\n",
    "    \n",
    "    idx2word_title = list(word_counts_title.keys())\n",
    "    idx2word_title.insert(0,\"ZERO\")\n",
    "    idx2word_title.append(\"UNK\")\n",
    "    \n",
    "    # Create word2idx\n",
    "    word2idx = Counter(word_counts_article) + Counter(word_counts_title)\n",
    "    word2idx[\"UNK\"] = 2\n",
    "    word2idx[\"ZERO\"] = 2\n",
    "    word2idx = word2idx.most_common()\n",
    "    word2idx = [i[0] for i in word2idx]\n",
    "    #values = word2idx.fetchall()\n",
    "    #word2idx = [word2idx[0] for word2idx in rows]\n",
    "    #word2idx = dict(word2idx)\n",
    "    #word2idx = collections.OrderedDict(word2idx)\n",
    "    #word2idx = OrderedDict(sorted(word2idx.items(), key=lambda word2idx: word2idx[1]))\n",
    "    #list(word2idx.keys()).index(key)\n",
    "    \n",
    "    # \n",
    "    #idx2word_article = collections.OrderedDict(idx2word_article)\n",
    "    #idx2word_title = collections.OrderedDict(idx2word_title)\n",
    "    \n",
    "    transformed_article = {}    \n",
    "    transformed_title = {}\n",
    "    \n",
    "    #i = 0\n",
    "    #transformed_article[\"UNK\"] = word2idx.index(\"UNK\")\n",
    "    #for key in idx2word_article:\n",
    "    #    if key in word2idx:\n",
    "    #        transformed_article[key] = word2idx.index(key)\n",
    "    #    else:\n",
    "    #        transformed_article[key] = transformed_article[\"UNK\"]\n",
    "    \n",
    "    transformed_article = []\n",
    "    with open(article) as f_article:\n",
    "        for line in f_article:\n",
    "            newline = []\n",
    "            words_line = re.findall(r'\\w+', line)\n",
    "            for word in words_line:\n",
    "                if word in word2idx:\n",
    "                    newline.append(word2idx.index(word))\n",
    "            transformed_article.append(newline)\n",
    "    \n",
    "    transformed_title = []\n",
    "    with open(summary) as f_title:\n",
    "        for line in f_title:\n",
    "            newline = []\n",
    "            words_line = re.findall(r'\\w+', line)\n",
    "            for word in words_line:\n",
    "                if word in word2idx:\n",
    "                    newline.append(word2idx.index(word))\n",
    "            transformed_title.append(newline)\n",
    "    \n",
    "    print(transformed_article[0])\n",
    "    print(transformed_title[0])\n",
    "\n",
    "#   Return the following outputs:\n",
    "#1. transformed article data\n",
    "#2. vocab size of article data\n",
    "#3. idx2word(articledata)\n",
    "#4. word2idx(articledata)\n",
    "#5. transformed summary data\n",
    "#6. vocab size of summary data\n",
    "#7. idx2word(summarydata)\n",
    "#8. word2idx(summarydata)\n",
    "    \n",
    "    return transformed_article, vocab_size, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the above function to load the training data from article and summary (i.e. title) files. Do note that, based on your model architecture, you may need to further one-hot vectorize your input to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO\n",
    "\n",
    "article_file = \"data/train_article.txt\"\n",
    "title_file = \"data/train_title.txt\"\n",
    "load_data(article_file, title_file, MAX_LEN, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unidirectional LSTM Encoder Decoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parameters for your LSTM encoder decoder model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = \n",
    "NUM_LAYERS = \n",
    "HIDDEN_DIM = \n",
    "EPOCHS ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Unidirectional encoder decoder LSTM model in create_model function. The model should have a LSTM Unidirectional layer as encoder and a LSTM decoder.\n",
    "Use categorical_cross_entropy loss and experiment with different optimizers to improve your score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_UniLSTM(X_vocab_len, X_max_len, y_vocab_len, y_max_len, hidden_size, num_layers):\n",
    "    # TO-DO\n",
    "    # create and return the model for unidirectional LSTM encoder decoder\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have everything in place, we can run our model. We recommend training the model in batches instead of training on all 50,000 article-title pairs at once, if you encounter memory contraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using Rouge score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have trained the model, load the test data i.e. test_article.txt and corresponding reference titles test_title.txt. Process test_article.txt in the same way as you did your train_article.txt. Then use your model to predict the titles.\n",
    "When you have your model predicted titles, and the reference titles (test_title.txt) calculate the Rouge score corresponding to your predictions. <br>\n",
    "You should install rouge by executing \"pip3 install rouge\". Refer https://pypi.python.org/pypi/rouge/0.2.1 for documentation on how to use the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommended training the data in batches because of our tensor constraints. This also presents us with a challenge of visualizing loss function and accuracy change with each epoch. Keras has an inbuilt function called fit_generator which takes in a generator function and gives the required batch for training. Use this Function to load data in batches of 100 for 200 steps_per_epoch. Run the training for 10 epochs. Use Keras callbacks to send data to tensorboad (you can look this up online). \n",
    "\n",
    "Once your training is done. Go to command line and run tensorboard. By default Tensorboard opens on 6006 port. Do remember to allow traffic on the same for gcloud (like you did for previous assignment). You can see various metrics depending on what you want to track like loss, accuracy, validation loss and validation accuracy over epochs. Attach the plots of loss and accuracy from the tensorboard display in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unidirectional LSTM Encoder Decoder With Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parameters for your LSTM encoder decoder model with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = \n",
    "NUM_LAYERS = \n",
    "HIDDEN_DIM = \n",
    "EPOCHS ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would've observed that the summaries are not yet perfect. This is because in encoder decoder architecture, only the final state of encoder is used to calculate the probabilities. We now move to a more general approach called attention based approach. In this, we take a weighted sum of all weights of encoder instead of just the last one. You are already provided an attention_decoder.py file with AttentionDecoder. Add this layer on top of your encoder and run the same experiment as before. For this part, you don't need to worry about return_probabilities argument to create_UniLSTMwithAttention function. Just pass it as an argument to your attention decoder layer. When return_probabilities is false, the attention decoder returns prediction model, which is what you need for this part of the assignment. When return_probabilities is true, the attention decoder returns the probability model, which you will be using later in the Analysis part of this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_UniLSTMwithAttention(X_vocab_len, X_max_len, y_vocab_len, y_max_len, hidden_size, num_layers, return_probabilities = False):\n",
    "    # TO-DO\n",
    "    # create and return the model for unidirectional LSTM encoder decoder with attention\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model, as you did before, for the model without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using Rouge Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your model as before, using Rouge score. Ideally, your scores for the model with attention should be better than the model without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we evaluate our models on ROUGE score, we don't train our neural networks to learn better ROUGE score for the fact that ROUGE score is a complicated nonconvex function. How does our model learn then? In information theory, Perplexity is a measure of how good a model is.\n",
    "\n",
    "Perplexity                     $$ = 2^{{-\\sum _{x}p(x)\\log _{2}p(x)}}$$ \n",
    "            \n",
    "Lower the perplexity, better the model. Justify why our model learns well with our loss function? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now plot the attention weights for a sentence and it's output. If a grid cell is white in the plot, it means that during summary, the word on x-axis corresponds to the word on y-axis. You are provided with a Visualizer class for helping you out. Make sure you install matplotlib using sudo pip3 install matplotlib and also install python3-tk using sudo apt-get install python3-tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualizer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            Visualizes attention maps\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    def set_models(self, pred_model, proba_model):\n",
    "        \"\"\"\n",
    "            Sets the models to use\n",
    "            :param pred_model: the prediction model\n",
    "            :param proba_model: the model that outputs the activation maps\n",
    "        \"\"\"\n",
    "        self.pred_model = pred_model\n",
    "        self.proba_model = proba_model\n",
    "\n",
    "    def attention_map(self, text, padded_data_vec, y_idx_to_word):\n",
    "        \"\"\"\n",
    "            Displays the attention weights graph\n",
    "            param: input sentence\n",
    "            param: padded_data_vector for prediction\n",
    "            param: idx2word dictionary for titles\n",
    "        \"\"\"\n",
    "        input_length = len(text.split())\n",
    "        \n",
    "        # get the output sequence\n",
    "        prediction = np.argmax(pred_model.predict(padded_data_vec), axis=2)[0]\n",
    "        text_ = text.split()\n",
    "        valids = [y_idx_to_word[index] for index in prediction if index > 0]\n",
    "        sequence = ' '.join(valids)\n",
    "        predicted_text = sequence.split()\n",
    "        output_length = len(predicted_text)\n",
    "        #get the weights\n",
    "        activation_map = np.squeeze(self.proba_model.predict(padded_data_vec))[\n",
    "            0:output_length, 0:input_length]\n",
    "        \n",
    "        plt.clf()\n",
    "        f = plt.figure(figsize=(8, 8.5))\n",
    "        ax = f.add_subplot(1, 1, 1)\n",
    "\n",
    "        # add image\n",
    "        i = ax.imshow(activation_map, interpolation='nearest', cmap='gray')\n",
    "        \n",
    "        # add colorbar\n",
    "        cbaxes = f.add_axes([0.2, 0, 0.6, 0.03])\n",
    "        cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')\n",
    "        cbar.ax.set_xlabel('Probability', labelpad=2)\n",
    "\n",
    "        # add labels\n",
    "        ax.set_yticks(range(output_length))\n",
    "        ax.set_yticklabels(predicted_text[:output_length])\n",
    "        \n",
    "        ax.set_xticks(range(input_length))\n",
    "        ax.set_xticklabels(text_[:input_length], rotation=45)\n",
    "        \n",
    "        ax.set_xlabel('Input Sequence')\n",
    "        ax.set_ylabel('Output Sequence')\n",
    "\n",
    "        # add grid and legend\n",
    "        ax.grid()\n",
    "        \n",
    "        f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can initialize Visualizer class as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = Visualizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizer has two methods.\n",
    "- set_models \n",
    "- attention_map\n",
    "\n",
    "The set_models takes in prediction model and probability model as inputs. In *create_UniLSTMwithAttention*, the model with *return_probabilities = False* which you already used in the training is the prediction model. For initializing probability model, call *create_UniLSTMwithAttention* with *return_probabilities = True* and initialize the weights with weights of prediction model. Now you can call set_models in this manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viz.set_models(pred_model,prob_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention_map creates the weights map for you. You need to give a sample sentence, a test_data_vector on which we call call model.predict and your output idx2word dictionary. You can call it as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viz.attention_map(text,test_data_vector,idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above Visualizer to visualize attention weights for 15 sentences, as instructed in the Analysis section of the accompanying HW document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unidirectional LSTM Encoder Decoder With Attention and Beam Search (Extra Credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models that you implemented till now had greedy decoder. Now implement a Decoder with Beam Search and show improved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
